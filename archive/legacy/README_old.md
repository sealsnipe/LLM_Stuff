# ğŸš€ LLM Training System

Professional GPU-optimized LLM training system with advanced features.

## ğŸ“ Project Structure

```
LLM_Stuff/
â”œâ”€â”€ ğŸ¯ CORE TRAINING FILES
â”‚   â”œâ”€â”€ training-windows.py          # Main training script
â”‚   â”œâ”€â”€ config.py                    # Central configuration
â”‚   â”œâ”€â”€ professional_logger.py       # Professional logging system
â”‚   â”œâ”€â”€ debug_logger.py             # Debug & error logging
â”‚   â”œâ”€â”€ training_logger.py          # Training metrics logging
â”‚   â”œâ”€â”€ fast_dataset_loader.py      # Optimized dataset loading
â”‚   â”œâ”€â”€ sequence_packing_cache.py   # Sequence packing & caching
â”‚   â”œâ”€â”€ optimized_sequence_packing.py # Sequence packing algorithms
â”‚   â””â”€â”€ muon_optimizer.py           # Advanced optimizers
â”‚
â”œâ”€â”€ ğŸ“‚ ORGANIZED DIRECTORIES
â”‚   â”œâ”€â”€ tests/                      # All test files
â”‚   â”œâ”€â”€ scripts/                    # Utility scripts & .bat files
â”‚   â”œâ”€â”€ reports/                    # Performance reports & documentation
â”‚   â”œâ”€â”€ archive/                    # Old/deprecated files
â”‚   â”œâ”€â”€ docs/                       # Technical documentation
â”‚   â”œâ”€â”€ cache/                      # Dataset & sequence caches
â”‚   â”œâ”€â”€ training_logs/              # Training run logs
â”‚   â””â”€â”€ logs/                       # Analysis plots & charts
```

## ğŸš€ Quick Start

### 1. Start Training
```bash
python training-windows.py
```

### 2. Configuration
Edit `config.py` for:
- Model architecture (372M parameters)
- Training parameters (batch size, learning rate)
- Dataset settings (FineWeb-Edu)
- Hardware optimizations

### 3. Debug Mode
```python
# In config.py:
debug_mode: bool = True  # Enable detailed debug logs
```

## âš¡ Key Features

### ğŸ¯ **Training System**
- **GPU-Optimized**: RTX 3090/4090 optimized
- **Mixed Precision**: BFLOAT16 training
- **Flash Attention**: Memory-efficient attention
- **Torch Compile**: JIT compilation for speed
- **Sequence Packing**: 99% token utilization

### ğŸ“Š **Monitoring**
- **Professional Logs**: Clean console output
- **Debug System**: Detailed debug files
- **Training Metrics**: Real-time performance tracking
- **Progress Display**: ETA, tokens/sec, loss tracking

### ğŸ’¾ **Data Pipeline**
- **Packed Cache**: Instant loading (no tokenization)
- **Intelligent Training**: Auto-adjusts steps based on data
- **FineWeb-Edu**: High-quality training data
- **Resume Support**: Crash-resistant training

### ğŸ”§ **Advanced Features**
- **Checkpoint System**: Auto-save & resume
- **Memory Optimization**: Gradient checkpointing
- **Multi-Worker**: Parallel data loading
- **Error Handling**: Robust error recovery

## ğŸ“ˆ Performance

**Typical Performance (RTX 3090):**
- **Speed**: ~9,000-12,000 tokens/sec
- **Memory**: ~20-22GB VRAM usage
- **Efficiency**: 99% token utilization
- **Stability**: <1% gradient clipping

## ğŸ› ï¸ Directory Details

### `/tests/`
- Unit tests for all components
- Integration tests
- Performance benchmarks
- Interactive chat tests

### `/scripts/`
- Dataset download scripts
- Cache creation utilities
- Analysis tools
- Batch files for automation

### `/reports/`
- Performance analysis reports
- Optimization documentation
- Sequence packing studies

### `/archive/`
- Deprecated code
- Old implementations
- Legacy training scripts

## ğŸ” Troubleshooting

### Common Issues
1. **GPU Memory**: Reduce batch_size in config.py
2. **Slow Training**: Enable debug_mode to check bottlenecks
3. **Cache Issues**: Delete cache/ and regenerate
4. **Import Errors**: Ensure all core files are in root directory

### Debug Files
- `debug_run_XXX.log` - Detailed debug information
- `error_run_XXX.log` - Error logs and stack traces
- `training_logs/` - Training metrics and plots

## ğŸ“ Configuration

Key settings in `config.py`:

```python
# Model Architecture
hidden_size: int = 1536          # Model dimension
num_layers: int = 28             # Transformer layers
num_attention_heads: int = 12    # Attention heads

# Training Parameters
batch_size: int = 12             # GPU-optimized batch size
gradient_accumulation_steps: int = 6  # Effective batch: 72
sequence_length: int = 768       # Context length
learning_rate: float = 2.55e-4   # Optimized learning rate

# Performance
use_torch_compile: bool = True   # JIT compilation
use_mixed_precision: bool = True # BFLOAT16
use_flash_attention: bool = True # Memory efficiency
debug_mode: bool = False         # Debug logging
```

## ğŸ¯ Next Steps

1. **Start Training**: Run `python training-windows.py`
2. **Monitor Progress**: Watch console output
3. **Check Logs**: Review `training_logs/` for metrics
4. **Optimize**: Adjust config.py based on performance
5. **Scale Up**: Increase batch_size if GPU allows

---

**Built for professional LLM training with maximum efficiency and reliability.** ğŸš€
