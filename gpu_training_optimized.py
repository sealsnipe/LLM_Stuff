# %% [markdown]
# # GPU-Optimiertes LLM Training
#
# Diese Version ist speziell f√ºr GPU-Training optimiert mit:
# - Automatische GPU-Erkennung
# - Mixed Precision Training (FP16)
# - Optimierte Batch-Gr√∂√üen f√ºr GPU
# - Memory-effiziente Implementierung

# %%
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.utils.checkpoint import checkpoint
import torch.nn.functional as F
import math
from typing import Dict, Optional
from dataclasses import dataclass
from tqdm import tqdm
import time
import sys
import gc
import psutil

# %%
# IMPROVED: State-of-the-Art Memory Optimizations
try:
    import deepspeed
    DEEPSPEED_AVAILABLE = True
except ImportError:
    DEEPSPEED_AVAILABLE = False

try:
    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
    from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy
    from torch.distributed.fsdp import CPUOffload, MixedPrecision
    FSDP_AVAILABLE = True
except ImportError:
    FSDP_AVAILABLE = False

class CPUOffloadOptimizer:
    """CPU-Offloading Optimizer f√ºr Memory-Effizienz."""

    def __init__(self, params, lr=1e-4, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01):
        self.param_groups = [{'params': list(params), 'lr': lr, 'betas': betas, 'eps': eps, 'weight_decay': weight_decay}]
        self.state = {}
        self.lr = lr
        self.betas = betas
        self.eps = eps
        self.weight_decay = weight_decay

        # CPU-Offloading Setup
        self.cpu_states = {}
        self._setup_cpu_offload()

    def _setup_cpu_offload(self):
        """Initialisiere CPU-Offloading f√ºr Optimizer States."""
        for group in self.param_groups:
            for p in group['params']:
                if p.requires_grad:
                    param_id = id(p)
                    # Erstelle CPU-Kopien der Optimizer States
                    self.cpu_states[param_id] = {
                        'exp_avg': torch.zeros_like(p.data, device='cpu'),
                        'exp_avg_sq': torch.zeros_like(p.data, device='cpu'),
                        'step': 0
                    }

    def step(self):
        """Optimizer Step mit CPU-Offloading."""
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue

                param_id = id(p)
                state = self.cpu_states[param_id]

                # Lade States von CPU zu GPU
                exp_avg = state['exp_avg'].to(p.device)
                exp_avg_sq = state['exp_avg_sq'].to(p.device)

                state['step'] += 1

                # AdamW Update
                grad = p.grad.data
                if self.weight_decay != 0:
                    grad = grad.add(p.data, alpha=self.weight_decay)

                # Exponential moving average of gradient values
                exp_avg.mul_(self.betas[0]).add_(grad, alpha=1 - self.betas[0])

                # Exponential moving average of squared gradient values
                exp_avg_sq.mul_(self.betas[1]).addcmul_(grad, grad, value=1 - self.betas[1])

                # Bias correction
                bias_correction1 = 1 - self.betas[0] ** state['step']
                bias_correction2 = 1 - self.betas[1] ** state['step']

                step_size = self.lr / bias_correction1
                bias_correction2_sqrt = math.sqrt(bias_correction2)

                # Update parameters
                denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(self.eps)
                p.data.addcdiv_(exp_avg, denom, value=-step_size)

                # Speichere States zur√ºck auf CPU
                state['exp_avg'].copy_(exp_avg.cpu())
                state['exp_avg_sq'].copy_(exp_avg_sq.cpu())

                # Cleanup GPU Memory
                del exp_avg, exp_avg_sq

    def zero_grad(self, set_to_none=False):
        """Setze Gradienten zur√ºck."""
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is not None:
                    if set_to_none:
                        p.grad = None
                    else:
                        p.grad.detach_()
                        p.grad.zero_()

# %%
class MemoryMonitor:
    """Memory-Monitor f√ºr GPU und CPU."""

    def __init__(self):
        self.peak_gpu_memory = 0
        self.peak_cpu_memory = 0

    def get_memory_stats(self):
        """Aktuelle Memory-Statistiken."""
        # GPU Memory
        if torch.cuda.is_available():
            gpu_allocated = torch.cuda.memory_allocated() / 1e9
            gpu_reserved = torch.cuda.memory_reserved() / 1e9
            gpu_max = torch.cuda.max_memory_allocated() / 1e9
            self.peak_gpu_memory = max(self.peak_gpu_memory, gpu_allocated)
        else:
            gpu_allocated = gpu_reserved = gpu_max = 0

        # CPU Memory
        cpu_memory = psutil.virtual_memory()
        cpu_used = cpu_memory.used / 1e9
        cpu_percent = cpu_memory.percent
        self.peak_cpu_memory = max(self.peak_cpu_memory, cpu_used)

        return {
            'gpu_allocated': gpu_allocated,
            'gpu_reserved': gpu_reserved,
            'gpu_max': gpu_max,
            'gpu_peak': self.peak_gpu_memory,
            'cpu_used': cpu_used,
            'cpu_percent': cpu_percent,
            'cpu_peak': self.peak_cpu_memory
        }

    def cleanup_memory(self):
        """Aggressive Memory Cleanup."""
        # GPU Cleanup
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

        # CPU Cleanup
        gc.collect()

    def print_memory_stats(self, prefix=""):
        """Drucke Memory-Statistiken."""
        stats = self.get_memory_stats()
        print(f"{prefix}GPU: {stats['gpu_allocated']:.1f}GB allocated, {stats['gpu_peak']:.1f}GB peak")
        print(f"{prefix}CPU: {stats['cpu_used']:.1f}GB used ({stats['cpu_percent']:.1f}%)")

# %%
@dataclass
class GPUTrainingConfig:
    """1B Parameter Training - REALISTISCH f√ºr RTX 4070 Ti (12GB)."""
    # Model parameters - 1B Parameter (passt sicher auf 12GB)
    vocab_size: int = 32000
    hidden_size: int = 1536        # Zur√ºck zu 1B Parameter f√ºr GPU-Auslastung
    num_attention_heads: int = 24  # 64 dim per head
    num_key_value_heads: int = 6   # GQA 4:1 ratio f√ºr Memory-Effizienz
    num_layers: int = 12           # Mehr Layer f√ºr bessere GPU-Auslastung
    max_position_embeddings: int = 2048  # OK f√ºr Memory
    
    # Training parameters - 3B Model mit Memory-Optimierungen
    max_steps: int = 2000    # Kurzer Test f√ºr PoC
    batch_size: int = 5    # Sicher f√ºr 12GB VRAM
    gradient_accumulation_steps: int = 2   # Effektive batch_size = 24

    # Zus√§tzliche Memory-Optimierungen
    tie_word_embeddings: bool = True      # Teile Embedding/Output Weights
    learning_rate: float = 1e-4
    weight_decay: float = 0.1
    warmup_steps: int = 2000
    
    # Memory-Optimierungen f√ºr RTX 4070 Ti (12GB)
    use_mixed_precision: bool = True      # BF16 f√ºr Memory-Effizienz
    use_gradient_checkpointing: bool = False # AUS f√ºr Performance
    use_cpu_offload: bool = False         # Standard GPU Optimizer f√ºr Performance
    use_activation_checkpointing: bool = True  # Zus√§tzliche Aktivations-Optimierung
    use_zero_optimizer: bool = True       # ZeRO-√§hnliche Optimierungen
    use_torch_compile: bool = True        # Aktiviert f√ºr Performance
    max_grad_norm: float = 1.0
    sequence_length: int = 384            # Balance zwischen Performance und VRAM
    log_interval: int = 5                 # Logging alle 5 Steps
    
    # Memory-Optimierungen
    gradient_checkpointing: bool = True
    use_flash_attention: bool = True
    
    # Logging
    log_interval: int = 100
    eval_interval: int = 1000
    save_interval: int = 5000

def check_gpu_setup():
    """√úberpr√ºft GPU-Setup und gibt Empfehlungen."""
    print("=== GPU SETUP CHECK ===")
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA nicht verf√ºgbar!")
        print("üí° F√ºr LLM-Training brauchst du eine NVIDIA GPU mit CUDA")
        print("üîß Installiere: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        return False
    
    gpu_count = torch.cuda.device_count()
    print(f"‚úÖ CUDA verf√ºgbar: {torch.version.cuda}")
    print(f"üñ•Ô∏è  GPUs gefunden: {gpu_count}")
    
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        memory_gb = props.total_memory / 1e9
        print(f"   GPU {i}: {props.name}")
        print(f"   Memory: {memory_gb:.1f} GB")
        print(f"   Compute: {props.major}.{props.minor}")
        
        # Memory-Empfehlungen
        if memory_gb < 8:
            print(f"   ‚ö†Ô∏è  Wenig VRAM - reduziere batch_size")
        elif memory_gb < 16:
            print(f"   ‚úÖ Ausreichend f√ºr mittlere Modelle")
        else:
            print(f"   üöÄ Perfekt f√ºr gro√üe Modelle")
    
    return True

# %%
class GPUOptimizedAttention(nn.Module):
    """GPU-optimierte Attention mit Memory-Effizienz."""
    
    def __init__(self, config: GPUTrainingConfig):
        super().__init__()
        self.config = config  # Speichere Config f√ºr Memory-Optimierungen
        self.hidden_size = config.hidden_size
        self.num_attention_heads = config.num_attention_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.head_dim = config.hidden_size // config.num_attention_heads
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = 1.0 / math.sqrt(self.head_dim)

        # Projections - optimiert f√ºr GPU
        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)

        # Dropout f√ºr Training
        self.attention_dropout = nn.Dropout(0.1)
    
    def _attention_forward(self, hidden_states, attention_mask=None):
        """Interne Attention-Berechnung f√ºr Memory-Optimierung."""
        batch_size, seq_len, _ = hidden_states.shape

        # Projections
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        # Reshape f√ºr Multi-Head
        query_states = query_states.view(batch_size, seq_len, self.num_attention_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        # GQA: Repeat K/V heads
        if self.num_key_value_groups > 1:
            key_states = key_states.repeat_interleave(self.num_key_value_groups, dim=1)
            value_states = value_states.repeat_interleave(self.num_key_value_groups, dim=1)

        # üöÄ FLASH ATTENTION 2 - Automatisch optimiert in PyTorch 2.5+
        attn_output = torch.nn.functional.scaled_dot_product_attention(
            query_states,
            key_states,
            value_states,
            attn_mask=attention_mask,
            dropout_p=0.1 if self.training else 0.0,
            is_causal=True,  # Causal mask f√ºr LLM
            scale=self.scaling
        )

        # Reshape und Output
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, seq_len, self.hidden_size)

        return self.o_proj(attn_output)

    def forward(self, hidden_states, attention_mask=None):
        # üîß MEMORY OPTIMIZATION: Verwende Gradient Checkpointing wenn aktiviert
        if hasattr(self.config, 'use_activation_checkpointing') and self.config.use_activation_checkpointing and self.training:
            return checkpoint(self._attention_forward, hidden_states, attention_mask, use_reentrant=False)
        else:
            return self._attention_forward(hidden_states, attention_mask)

# %%
class MemoryOptimizedTransformerBlock(nn.Module):
    """Memory-optimierter Transformer Block mit Checkpointing."""

    def __init__(self, config: GPUTrainingConfig):
        super().__init__()
        self.config = config
        self.attention = GPUOptimizedAttention(config)

        # Feed Forward - SwiGLU f√ºr bessere Performance
        self.gate_proj = nn.Linear(config.hidden_size, config.hidden_size * 4, bias=False)
        self.up_proj = nn.Linear(config.hidden_size, config.hidden_size * 4, bias=False)
        self.down_proj = nn.Linear(config.hidden_size * 4, config.hidden_size, bias=False)

        # Layer Norms
        self.attention_norm = nn.LayerNorm(config.hidden_size, eps=1e-5)
        self.ffn_norm = nn.LayerNorm(config.hidden_size, eps=1e-5)

    def _ffn_forward(self, hidden_states):
        """Interne FFN-Berechnung f√ºr Checkpointing."""
        # SwiGLU Activation
        gate = torch.nn.functional.silu(self.gate_proj(hidden_states))
        up = self.up_proj(hidden_states)
        return self.down_proj(gate * up)

    def _block_forward(self, hidden_states, attention_mask=None):
        """Kompletter Block f√ºr Checkpointing."""
        # Pre-norm attention
        normed_states = self.attention_norm(hidden_states)
        attn_output = self.attention(normed_states, attention_mask)
        hidden_states = hidden_states + attn_output

        # Pre-norm feed forward
        normed_states = self.ffn_norm(hidden_states)
        if hasattr(self.config, 'use_activation_checkpointing') and self.config.use_activation_checkpointing and self.training:
            ffn_output = checkpoint(self._ffn_forward, normed_states, use_reentrant=False)
        else:
            ffn_output = self._ffn_forward(normed_states)
        hidden_states = hidden_states + ffn_output

        return hidden_states

    def forward(self, hidden_states, attention_mask=None):
        # üîß MEMORY OPTIMIZATION: Gradient Checkpointing f√ºr ganzen Block
        if hasattr(self.config, 'use_gradient_checkpointing') and self.config.use_gradient_checkpointing and self.training:
            return checkpoint(self._block_forward, hidden_states, attention_mask, use_reentrant=False)
        else:
            return self._block_forward(hidden_states, attention_mask)

# %%
class MemoryOptimizedLLM(nn.Module):
    """1B Parameter LLM - REALISTISCH f√ºr RTX 4070 Ti (12GB)."""

    def __init__(self, config: GPUTrainingConfig):
        super().__init__()
        self.config = config

        # Embeddings
        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)

        # Transformer layers - verwende memory-optimierte Bl√∂cke
        self.layers = nn.ModuleList([
            MemoryOptimizedTransformerBlock(config) for _ in range(config.num_layers)
        ])

        # Output
        self.norm = nn.LayerNorm(config.hidden_size, eps=1e-5)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Weight tying f√ºr Memory-Effizienz
        if hasattr(config, 'tie_word_embeddings') and config.tie_word_embeddings:
            self.lm_head.weight = self.token_embeddings.weight

        # Weight initialization
        self.apply(self._init_weights)

        print(f"üîß Memory-Optimized 1B LLM initialisiert:")
        print(f"   Hidden Size: {config.hidden_size}")
        print(f"   Layers: {config.num_layers}")
        print(f"   Attention Heads: {config.num_attention_heads}")
        print(f"   KV Heads: {config.num_key_value_heads}")
        total_params = sum(p.numel() for p in self.parameters())
        print(f"   Total Parameters: {total_params:,} ({total_params/1e9:.2f}B)")
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(self, input_ids, labels=None):
        batch_size, seq_len = input_ids.shape
        
        # Token embeddings
        hidden_states = self.token_embeddings(input_ids)
        
        # Causal mask
        attention_mask = torch.triu(
            torch.full((seq_len, seq_len), float('-inf'), device=input_ids.device),
            diagonal=1
        ).unsqueeze(0).unsqueeze(0)
        
        # Transformer layers - Memory-optimiert
        for layer in self.layers:
            # Gradient Checkpointing ist bereits in den Layern implementiert
            hidden_states = layer(hidden_states, attention_mask)
        
        # Output
        hidden_states = self.norm(hidden_states)
        logits = self.lm_head(hidden_states)
        
        # Loss calculation
        loss = None
        if labels is not None:
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))
        
        return {"loss": loss, "logits": logits}

# %%
def create_gpu_optimized_dataset(config: GPUTrainingConfig, num_samples: int = 10000):
    """Erstellt GPU-optimierten Datensatz."""
    # Gr√∂√üere Sequenzen f√ºr GPU
    input_ids = torch.randint(0, config.vocab_size, (num_samples, config.sequence_length))
    labels = input_ids.clone()
    
    dataset = TensorDataset(input_ids, labels)

    # üìä OPTIMIERTER DATALOADER (2025 Techniken)
    return DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=4,           # ‚Üê Parallel data loading
        pin_memory=True,         # ‚Üê GPU Memory Pinning
        persistent_workers=True, # ‚Üê Worker Persistence f√ºr Speedup
        prefetch_factor=4,       # ‚Üê Prefetch mehr Batches
        drop_last=True           # ‚Üê Konsistente Batch-Gr√∂√üen
    )

# %%
def memory_optimized_training_loop(config: GPUTrainingConfig):
    """3B Parameter Training mit extremen Memory-Optimierungen."""

    # Expliziter Import um Konflikte zu vermeiden
    import torch

    # GPU Setup Check
    if not check_gpu_setup():
        print("üö´ GPU Training nicht m√∂glich - verwende CPU-Version")
        return

    device = torch.device("cuda")
    print(f"üöÄ 1B Memory-Optimized Training: {config.max_steps} steps, micro-batch {config.batch_size}, grad-accum {config.gradient_accumulation_steps}")

    # Memory Monitor
    memory_monitor = MemoryMonitor()
    memory_monitor.print_memory_stats("üîß Initial Memory: ")

    # Model
    model = MemoryOptimizedLLM(config).to(device)

    # Memory Check nach Model Loading
    memory_monitor.print_memory_stats("üìä After Model Loading: ")

    # üöÄ OPTIMIZER mit CPU-Offloading
    if config.use_cpu_offload:
        print("üîß Verwende CPU-Offload Optimizer...")
        optimizer = CPUOffloadOptimizer(
            model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
    else:
        print("üîß Verwende Standard AdamW...")
        optimizer = optim.AdamW(
            model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay,
            betas=(0.9, 0.95),  # Bessere Werte f√ºr LLMs
            fused=True          # üöÄ FUSED f√ºr 10-20% Speedup
        )

    memory_monitor.print_memory_stats("üîß After Optimizer: ")

    # üöÄ TORCH.COMPILE mit MINIMAL LOGGING (optional f√ºr 3B)
    if hasattr(config, 'use_torch_compile') and config.use_torch_compile:
        import os
        import logging

        # Triton Cache Fix f√ºr Windows
        os.environ['TRITON_CACHE_DIR'] = os.path.join(os.environ.get('TEMP', 'C:\\temp'), 'triton_cache')

        # LOGGING REDUZIEREN
        logging.getLogger("triton").setLevel(logging.ERROR)
        logging.getLogger("torch._inductor").setLevel(logging.ERROR)
        logging.getLogger("torch._dynamo").setLevel(logging.ERROR)

        # AGGRESSIVE DYNAMO OPTIMIZATIONS
        import torch._dynamo
        torch._dynamo.config.suppress_errors = True
        torch._dynamo.config.verbose = False
        torch._dynamo.config.cache_size_limit = 1000  # Mehr Cache f√ºr bessere Performance
        print("üîß torch.compile wird aktiviert...")

        try:
            model = torch.compile(model, mode="max-autotune", fullgraph=False)
            print("‚úÖ torch.compile aktiviert")
        except Exception as e:
            print(f"‚ö†Ô∏è  torch.compile Fehler: {e}")
            print("üí° Fallback: Standard PyTorch")
    else:
        print("üîß torch.compile deaktiviert f√ºr Memory-Effizienz")

    # Optimizer bereits oben definiert - entferne Duplikat
    
    # üî• MODERNISIERTE MIXED PRECISION (PyTorch 2.5+)
    scaler = torch.amp.GradScaler('cuda') if config.use_mixed_precision else None
    
    # Dataset
    dataloader = create_gpu_optimized_dataset(config)
    data_iter = iter(dataloader)
    
    # Training state
    model.train()
    total_loss = 0.0
    step = 0

    # üìä PERFORMANCE MONITORING
    step_times = []
    start_time = time.time()
    
    print("üéØ Training gestartet...")

    # TRAINING LOOP mit minimaler Progress Bar
    progress_bar = tqdm(
        total=config.max_steps,
        desc="üöÄ Training",
        unit="step",
        ncols=120,
        bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] {postfix}",
        mininterval=2.0,  # Update nur alle 2 Sekunden f√ºr weniger Overhead
        maxinterval=10.0  # Maximal alle 10 Sekunden
    )

    while step < config.max_steps:
        step_start_time = time.time()
        accumulated_loss = 0.0
        
        # Gradient accumulation
        for micro_step in range(config.gradient_accumulation_steps):
            try:
                batch = next(data_iter)
            except StopIteration:
                data_iter = iter(dataloader)
                batch = next(data_iter)
            
            # üöÄ NON-BLOCKING GPU Transfer f√ºr bessere Performance
            input_ids, labels = [x.to(device, non_blocking=True) for x in batch]

            # üî• BFLOAT16 Mixed Precision (stabiler als FP16)
            if config.use_mixed_precision:
                with torch.amp.autocast('cuda', dtype=torch.bfloat16):  # ‚Üê Modernisierte API
                    outputs = model(input_ids, labels=labels)
                    loss = outputs["loss"] / config.gradient_accumulation_steps

                scaler.scale(loss).backward()
            else:
                outputs = model(input_ids, labels=labels)
                loss = outputs["loss"] / config.gradient_accumulation_steps
                loss.backward()
            
            accumulated_loss += loss.item()

            # üîß MEMORY CLEANUP nach jedem Micro-Step
            del input_ids, labels, outputs, loss
            if micro_step % 8 == 0:  # Cleanup alle 8 Micro-Steps
                memory_monitor.cleanup_memory()

        # üî• OPTIMIERTER OPTIMIZER STEP
        if config.use_mixed_precision:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
            scaler.step(optimizer)
            scaler.update()
        else:
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
            optimizer.step()

        # ‚ö° set_to_none f√ºr bessere Performance
        optimizer.zero_grad(set_to_none=True)
        
        step += 1
        total_loss += accumulated_loss

        # üìä PERFORMANCE TRACKING
        step_time = time.time() - step_start_time
        step_times.append(step_time)

        # Update Progress Bar
        progress_bar.update(1)

        # Logging
        if step % config.log_interval == 0:
            avg_loss = total_loss / config.log_interval
            gpu_memory = torch.cuda.max_memory_allocated() / 1e9
            lr = optimizer.param_groups[0]['lr']

            # üöÄ PERFORMANCE METRICS
            recent_step_times = step_times[-config.log_interval:] if len(step_times) >= config.log_interval else step_times
            avg_step_time = sum(recent_step_times) / len(recent_step_times) if recent_step_times else 0
            steps_per_sec = 1.0 / avg_step_time if avg_step_time > 0 else 0

            # Update progress bar with current stats
            progress_bar.set_postfix({
                'Loss': f'{avg_loss:.4f}',
                'GPU': f'{gpu_memory:.1f}GB',
                'Step/s': f'{steps_per_sec:.2f}',
                'Time': f'{avg_step_time:.1f}s'
            })

            total_loss = 0.0
            torch.cuda.reset_peak_memory_stats()
        
        # Kein Early Stopping - laufe die vollen Steps

    # Close progress bar
    progress_bar.close()

    print(f"\n‚úÖ Training completed!")
    print(f"üìä Final Stats:")
    print(f"   Steps: {step}")
    print(f"   GPU Memory Peak: {torch.cuda.max_memory_allocated() / 1e9:.1f}GB")
    print(f"   Model Parameters: {sum(p.numel() for p in model.parameters()):,}")

# %%
if __name__ == "__main__":
    # Verwende die 3B Memory-Optimized Konfiguration
    config = GPUTrainingConfig()
    
    print("=== GPU-OPTIMIERTE TRAINING KONFIGURATION ===")
    print(f"Max Steps: {config.max_steps}")
    print(f"Batch Size: {config.batch_size}")
    print(f"Gradient Accumulation: {config.gradient_accumulation_steps}")
    print(f"Effective Batch Size: {config.batch_size * config.gradient_accumulation_steps}")
    print(f"Mixed Precision: {config.use_mixed_precision}")
    print(f"Gradient Checkpointing: {config.gradient_checkpointing}")
    print()
    
    # Start Memory-Optimized 3B Training
    memory_optimized_training_loop(config)
