#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test-Skript fÃ¼r Model-Speicherung und -Validierung.

Dieses Skript testet die kritischen Model-Speicherungsfunktionen
ohne ein vollstÃ¤ndiges Training durchzufÃ¼hren.
"""

import torch
import torch.nn as nn
import os
import sys
import time
from datetime import datetime

# Import der notwendigen Komponenten
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import model_config, training_config

# Import der Klassen aus training-windows.py
import importlib.util
spec = importlib.util.spec_from_file_location("training_windows", "training-windows.py")
training_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(training_module)

MemoryOptimizedLLM = training_module.MemoryOptimizedLLM
save_trained_model = training_module.save_trained_model
validate_saved_model = training_module.validate_saved_model

def test_model_saving():
    """Testet Model-Speicherung mit einem kleinen Dummy-Training."""
    
    print("ğŸ§ª TEST: Model-Speicherung und -Validierung")
    print("=" * 60)
    
    # 1. Erstelle Test-Modell
    print("\n1ï¸âƒ£ Erstelle Test-Modell...")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"   Device: {device}")
    
    model = MemoryOptimizedLLM().to(device)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"   âœ… Modell erstellt: {total_params:,} Parameter")
    
    # 2. Simuliere kurzes Training
    print("\n2ï¸âƒ£ Simuliere Mini-Training (10 Steps)...")
    model.train()
    
    # Dummy-Optimizer
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    
    # Dummy-Training Loop
    start_time = time.time()
    total_loss = 0.0
    
    for step in range(1, 11):  # 10 Steps
        # Dummy-Daten
        input_ids = torch.randint(0, model_config.vocab_size, (2, 64), device=device)
        labels = input_ids.clone()
        
        # Forward Pass
        outputs = model(input_ids, labels=labels)
        loss = outputs['loss']
        
        # Backward Pass
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        total_loss += loss.item()
        
        if step % 5 == 0:
            avg_loss = total_loss / step
            print(f"   Step {step}/10: Loss = {avg_loss:.4f}")
    
    training_time = time.time() - start_time
    final_loss = total_loss / 10
    
    print(f"   âœ… Mini-Training abgeschlossen")
    print(f"   Trainingszeit: {training_time:.2f}s")
    print(f"   Finale Loss: {final_loss:.4f}")
    
    # 3. Teste Model-Speicherung
    print("\n3ï¸âƒ£ Teste Model-Speicherung...")
    
    try:
        # Erstelle Test-Verzeichnis
        test_dir = os.path.expanduser("~/AI/llm-coding/test_models")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_save_path = os.path.join(test_dir, f"test_model_{timestamp}")
        
        saved_path = save_trained_model(
            model=model,
            step=10,
            final_loss=final_loss,
            training_time=training_time,
            save_dir=model_save_path
        )
        
        print(f"   âœ… Model-Speicherung erfolgreich!")
        
    except Exception as e:
        print(f"   âŒ Model-Speicherung fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # 4. Teste Model-Validierung
    print("\n4ï¸âƒ£ Teste Model-Validierung...")
    
    try:
        validation_success = validate_saved_model(saved_path)
        
        if validation_success:
            print(f"   âœ… Model-Validierung erfolgreich!")
        else:
            print(f"   âŒ Model-Validierung fehlgeschlagen!")
            return False
            
    except Exception as e:
        print(f"   âŒ Model-Validierung Fehler: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # 5. Teste Model-Laden
    print("\n5ï¸âƒ£ Teste Model-Laden...")
    
    try:
        # Lade das gespeicherte Modell
        checkpoint_file = os.path.join(saved_path, 'model.pt')
        checkpoint = torch.load(checkpoint_file, map_location='cpu', weights_only=False)
        
        # Erstelle neues Modell und lade State
        new_model = MemoryOptimizedLLM()
        new_model.load_state_dict(checkpoint['model_state_dict'])
        new_model = new_model.to(device)
        new_model.eval()
        
        # Teste Inference
        test_input = torch.randint(0, model_config.vocab_size, (1, 10), device=device)
        
        with torch.no_grad():
            outputs = new_model(test_input)
            logits = outputs['logits']
        
        print(f"   âœ… Model erfolgreich geladen und getestet!")
        print(f"   Output Shape: {logits.shape}")
        
        # Vergleiche Parameter
        original_params = sum(p.numel() for p in model.parameters())
        loaded_params = sum(p.numel() for p in new_model.parameters())
        
        if original_params == loaded_params:
            print(f"   âœ… Parameter-Anzahl stimmt Ã¼berein: {loaded_params:,}")
        else:
            print(f"   âŒ Parameter-Anzahl unterschiedlich: {original_params} vs {loaded_params}")
            return False
        
    except Exception as e:
        print(f"   âŒ Model-Laden fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # 6. Teste gespeicherte Dateien
    print("\n6ï¸âƒ£ PrÃ¼fe gespeicherte Dateien...")
    
    expected_files = ['model.pt', 'config.json', 'training_info.json', 'README.md']
    
    for filename in expected_files:
        filepath = os.path.join(saved_path, filename)
        if os.path.exists(filepath):
            file_size = os.path.getsize(filepath)
            print(f"   âœ… {filename}: {file_size:,} bytes")
        else:
            print(f"   âŒ {filename}: Datei fehlt!")
            return False
    
    # 7. Zusammenfassung
    print("\n" + "=" * 60)
    print("ğŸ‰ ALLE TESTS ERFOLGREICH!")
    print(f"   Model gespeichert in: {saved_path}")
    print(f"   Bereit fÃ¼r produktives Training!")
    print("=" * 60)
    
    return True


def cleanup_test_models():
    """Bereinigt Test-Modelle (optional)."""
    
    test_dir = os.path.expanduser("~/AI/llm-coding/test_models")
    if os.path.exists(test_dir):
        import shutil
        try:
            shutil.rmtree(test_dir)
            print(f"ğŸ§¹ Test-Modelle bereinigt: {test_dir}")
        except Exception as e:
            print(f"âš ï¸  Bereinigung fehlgeschlagen: {e}")


if __name__ == "__main__":
    # Windows UTF-8 Fix
    if os.name == 'nt':
        os.system('chcp 65001 > nul')
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    
    print("ğŸ”§ LLM Model-Speicherung Test")
    print(f"Datum: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"PyTorch: {torch.__version__}")
    print(f"CUDA verfÃ¼gbar: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    # FÃ¼hre Tests aus
    success = test_model_saving()
    
    if success:
        print("\nâœ… Model-Speicherung ist bereit fÃ¼r produktives Training!")
        
        # Frage nach Bereinigung
        try:
            response = input("\nğŸ§¹ Test-Modelle lÃ¶schen? (y/N): ").strip().lower()
            if response in ['y', 'yes', 'ja', 'j']:
                cleanup_test_models()
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Test beendet.")
    else:
        print("\nâŒ Model-Speicherung muss behoben werden!")
        sys.exit(1)
